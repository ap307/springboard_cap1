{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec, Word2Vec\n",
    "\n",
    "# Setup nlkt\n",
    "import nltk.data\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_reviews(text):\n",
    "    encode = [w for w in word_tokenize(text) if not w.lower() in stopset]\n",
    "    encode = \" \".join(WordNetLemmatizer().lemmatize(lem_item, pos='n') for lem_item in encode)\n",
    "    # delete special characters except for apostraphes\n",
    "    encode = str(encode).translate(string.maketrans(\"\",\"\"), string.punctuation.replace(\"'\",\"\"))\n",
    "    encode = encode.replace(\"''\", \"\")\n",
    "    encode = encode.replace(\" ' \", \"\")\n",
    "    encode = encode.replace(\"' \", \"\")\n",
    "    # remove multipe spaces\n",
    "    encode = re.sub( '\\s+', ' ', encode).strip()\n",
    "    encode = encode.lower()\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pos review number 12499 of 12500\r"
     ]
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stopset = set(stopwords.words('english'))\n",
    "\n",
    "reviews = pd.DataFrame(columns=[\"reviews\", \"sentiment\"])\n",
    "\n",
    "sentiment_vec = [\"neg\", \"pos\"]\n",
    "\n",
    "base_directory = r\"C:\\Github\\springboard_cap1\\bayes\\data\"\n",
    "\n",
    "for item in sentiment_vec:\n",
    "    directory = base_directory + \"\\\\\" + item\n",
    "    num_files = len(os.walk(directory).next()[2])\n",
    "    for count, filename in enumerate(os.listdir(directory)):\n",
    "        print 'Reading {0} review number {1} of {2}\\r'.format(item, count, num_files),\n",
    "        if filename.endswith('.txt'):\n",
    "            filename_loc = directory + \"\\\\\" + filename\n",
    "            with open(filename_loc) as f:\n",
    "                paragraph = f.read().strip().decode('utf8')\n",
    "                # remove HTML tags\n",
    "                paragraph = re.sub('<[^<]+?>', ' ', paragraph)\n",
    "                # remove numbers\n",
    "                paragraph = re.sub(r'\\d+', '', paragraph)\n",
    "                #sentence_split = paragraph sent_detector.tokenize(paragraph)\n",
    "                #for sentence_count, sentence_item in enumerate(sentence_split):\n",
    "                #    reviews.loc[filename + \" \" + str(sentence_count), \"reviews\"] = encode_reviews(sentence_item)\n",
    "                #    reviews.loc[filename + \" \" + str(sentence_count), \"sentiment\"] = item\n",
    "                reviews.loc[filename, \"reviews\"] = encode_reviews(paragraph)\n",
    "                reviews.loc[filename, \"sentiment\"] = item\n",
    "            continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb     publication  \\\n",
       "1         Derek Adams  fresh  114709        Time Out   \n",
       "2     Richard Corliss  fresh  114709   TIME Magazine   \n",
       "3         David Ansen  fresh  114709        Newsweek   \n",
       "4       Leonard Klady  fresh  114709         Variety   \n",
       "5  Jonathan Rosenbaum  fresh  114709  Chicago Reader   \n",
       "\n",
       "                                               quote review_date  rtid  \\\n",
       "1  So ingenious in concept, design and execution ...  2009-10-04  9559   \n",
       "2                  The year's most inventive comedy.  2008-08-31  9559   \n",
       "3  A winning animated feature that has something ...  2008-08-18  9559   \n",
       "4  The film sports a provocative and appealing st...  2008-06-09  9559   \n",
       "5  An entertaining computer-generated, hyperreali...  2008-03-10  9559   \n",
       "\n",
       "       title  \n",
       "1  Toy story  \n",
       "2  Toy story  \n",
       "3  Toy story  \n",
       "4  Toy story  \n",
       "5  Toy story  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics = pd.read_csv('./critics.csv')\n",
    "#let's drop rows with missing quotes\n",
    "critics = critics[~critics.quote.isnull()]\n",
    "critics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_pd_index = critics.apply(lambda row: str(row[\"imdb\"]) + \" \"+ str(row[\"critic\"]), axis=1)\n",
    "rt_set = pd.DataFrame(index=new_pd_index)\n",
    "# remove tags\n",
    "rt_set[\"reviews\"] = np.asarray(critics[\"quote\"].str.replace(\"</\", \" \"))\n",
    "# remove numbers\n",
    "rt_set[\"reviews\"] = np.asarray(rt_set.apply(lambda x: re.sub(r'\\d+', '', x[\"reviews\"]), axis=1))\n",
    "# encode\n",
    "rt_set[\"reviews\"] = np.asarray(rt_set.apply(lambda x: encode_reviews(x[\"reviews\"]), axis=1))\n",
    "rt_set[\"sentiment\"] = np.asarray(critics[\"fresh\"].replace([\"fresh\", \"rotten\"], [\"pos\", \"neg\"] ))\n",
    "combined_set = pd.concat([reviews, rt_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.to_csv('imdb_set.csv')\n",
    "combined_set.to_csv('train_set.csv')\n",
    "rt_set.to_csv('rt_set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
